#Reading data
str(OJ)
head(OJ)
tail(OJ)
#help(OJ)
#Removing LoyalCH (column 10)
xdata = OJ[,-10]
head(xdata)
#Store ID could be recoded to a categorical variable
xdata$StoreID = as.factor(xdata$StoreID)
#Special variables are binary
xdata$SpecialCH = as.factor(xdata$SpecialCH)
xdata$SpecialMM = as.factor(xdata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
xdata = xdata[,-17]
#Observing changes
str(xdata)
#First, set random seed
set.seed(8655)
n = nrow(OJ)
#Drawing a sample of n/2 numbers ranging from 1 to n
ind = sample(1:n, size = floor(n/2))
#Dividing OJ into two datasets based on indices
train = OJ[ind,]
test = OJ[-ind,]
head(train)
head(test)
summary(xdata)
summary(xdata$Purchase)
summary(train)
summary(train$Purchase)
#Plotting Purchase-variable
plot(train$Purchase)
#Loading data
library(ISLR)
data(OJ)
#Reading data
str(OJ)
head(OJ)
tail(OJ)
#help(OJ)
#Removing LoyalCH (column 10)
xdata = OJ[,-10]
head(xdata)
#Store ID could be recoded to a categorical variable
xdata$StoreID = as.factor(xdata$StoreID)
#Special variables are binary
xdata$SpecialCH = as.factor(xdata$SpecialCH)
xdata$SpecialMM = as.factor(xdata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
xdata = xdata[,-17]
#Removing Store7 (column 13) for the same reason
xdata = xdata[,-13]
#Observing changes
str(xdata)
#First, set random seed
set.seed(8655)
n = nrow(OJ)
#Drawing a sample of n/2 numbers ranging from 1 to n
ind = sample(1:n, size = floor(n/2))
#Dividing OJ into two datasets based on indices
train = OJ[ind,]
test = OJ[-ind,]
head(train)
head(test)
summary(train)
summary(train$Purchase)
#Plotting Purchase-variable
plot(train$Purchase)
#First, set random seed
set.seed(8655)
n = nrow(OJ)
#Drawing a sample of n/2 numbers ranging from 1 to n
ind = sample(1:n, size = floor(n/2))
#Dividing OJ into two datasets based on indices
train = OJ[ind,]
test = OJ[-ind,]
head(train)
head(test)
#Loading data
library(ISLR)
data(OJ)
#Reading data
str(OJ)
head(OJ)
tail(OJ)
#help(OJ)
#Removing LoyalCH (column 10)
xdata = OJ[,-10]
head(xdata)
#Store ID could be recoded to a categorical variable
xdata$StoreID = as.factor(xdata$StoreID)
#Special variables are binary
xdata$SpecialCH = as.factor(xdata$SpecialCH)
xdata$SpecialMM = as.factor(xdata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
xdata = xdata[,-17]
#Removing Store7 (column 13) for the same reason
xdata = xdata[,-13]
#Observing changes
str(xdata)
#First, set random seed
set.seed(8655)
n = nrow(OJ)
#Drawing a sample of n/2 numbers ranging from 1 to n
ind = sample(1:n, size = floor(n/2))
#Dividing OJ into two datasets based on indices
train = xdata[ind,]
test = xdata[-ind,]
head(train)
head(test)
summary(train)
summary(train$Purchase)
#Plotting Purchase-variable
plot(train$Purchase)
summary(train)
plot(train$Purchase~train$WeekofPurchase)
boxplot(train$Purchase~train$WeekofPurchase)
plot(train$Purchase~train$WeekofPurchase)
par(mfrow=c(7,2))
plot(train$Purchase~train$WeekofPurchase)
par(mfrow=c(2,7))
plot(train$Purchase~train$WeekofPurchase)
plot(train$Purchase~train$StoreID)
plot(train$Purchase~train$PriceCH)
plot(train$Purchase~train$PriceMM)
plot(train$Purchase~train$DiscCH)
plot(train$Purchase~train$DiscMM)
plot(train$Purchase~train$SpecialCH)
plot(train$Purchase~train$SpecialMM)
plot(train$Purchase~train$SalePriceMM)
plot(train$Purchase~train$SalePriceCH)
plot(train$Purchase~train$PriceDiff)
plot(train$Purchase~train$PctDiscMM)
plot(train$Purchase~train$PctDiscCH)
plot(train$Purchase~train$ListPriceDiff)
par(mfrow=c(5,3))
plot(train$Purchase~train$WeekofPurchase)
par(mfrow=c(4,4))
plot(train$Purchase~train$WeekofPurchase)
plot(train$Purchase~train$StoreID)
plot(train$Purchase~train$PriceCH)
plot(train$Purchase~train$PriceMM)
plot(train$Purchase~train$DiscCH)
plot(train$Purchase~train$DiscMM)
plot(train$Purchase~train$SpecialCH)
plot(train$Purchase~train$SpecialMM)
plot(train$Purchase~train$SalePriceMM)
plot(train$Purchase~train$SalePriceCH)
plot(train$Purchase~train$PriceDiff)
plot(train$Purchase~train$PctDiscMM)
plot(train$Purchase~train$PctDiscCH)
plot(train$Purchase~train$ListPriceDiff)
table(train$Purchase~train$WeekofPurchase)
table(train$Purchase,train$WeekofPurchase)
table(train$Purchase,train$WeekofPurchase)
table(train$Purchase,train$StoreID)
table(train$Purchase,train$PriceCH)
table(train$Purchase,train$PriceMM)
table(train$Purchase,train$DiscCH)
table(train$Purchase,train$DiscMM)
table(train$Purchase,train$SpecialCH)
table(train$Purchase,train$SpecialMM)
table(train$Purchase,train$SalePriceMM)
table(train$Purchase,train$SalePriceCH)
table(train$Purchase,train$PriceDiff)
table(train$Purchase,train$PctDiscMM)
table(train$Purchase,train$PctDiscCH)
table(train$Purchase,train$ListPriceDiff)
plot(train$Purchase~train$WeekofPurchase)
plot(train$Purchase~train$StoreID)
plot(train$Purchase~train$PriceCH)
plot(train$Purchase~train$PriceMM)
plot(train$Purchase~train$DiscCH)
plot(train$Purchase~train$DiscMM)
plot(train$Purchase~train$SpecialCH)
plot(train$Purchase~train$SpecialMM)
plot(train$Purchase~train$SalePriceMM)
plot(train$Purchase~train$SalePriceCH)
plot(train$Purchase~train$PriceDiff)
plot(train$Purchase~train$PctDiscMM)
plot(train$Purchase~train$PctDiscCH)
plot(train$Purchase~train$ListPriceDiff)
#Building model on training data
log_reg1 = glm(Purchase~., data = train, family = binomial())
kable(tidy(log_reg1), "simple", digits = 3)
summary(tidy(log_reg1), "simple", digits = 3)
summary(log_reg1)
#Loading data
library(ISLR)
data(OJ)
#Reading data
str(OJ)
head(OJ)
tail(OJ)
#help(OJ)
#Removing LoyalCH (column 10)
xdata = OJ[,-10]
head(xdata)
#Removing potential NAs
#Store ID could be recoded to a categorical variable
xdata$StoreID = as.factor(xdata$StoreID)
#Special variables are binary
xdata$SpecialCH = as.factor(xdata$SpecialCH)
xdata$SpecialMM = as.factor(xdata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
xdata = xdata[,-17]
#Removing Store7 (column 13) for the same reason
xdata = xdata[,-13]
#Observing changes
str(xdata)
#Removing potential NAs
dim(xdata)
xdata = na.omit(xdata)
dim(xdata)
#First, set random seed
set.seed(8655)
n = nrow(OJ)
#Drawing a sample of n/2 numbers ranging from 1 to n
ind = sample(1:n, size = floor(n/2))
#Dividing OJ into two datasets based on indices
train = xdata[ind,]
test = xdata[-ind,]
head(train)
head(test)
#Loading data
library(ISLR)
data(OJ)
#Reading data
str(OJ)
head(OJ)
tail(OJ)
#help(OJ)
#Removing LoyalCH (column 10)
xdata = OJ[,-10]
head(xdata)
#Removing potential NAs
dim(xdata)
xdata = na.omit(xdata)
dim(xdata)
#Store ID could be recoded to a categorical variable
xdata$StoreID = as.factor(xdata$StoreID)
#Special variables are binary
xdata$SpecialCH = as.factor(xdata$SpecialCH)
xdata$SpecialMM = as.factor(xdata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
xdata = xdata[,-17]
#Removing Store7 (column 13) for the same reason
xdata = xdata[,-13]
#Observing changes
str(xdata)
#First, set random seed
set.seed(8655)
n = nrow(OJ)
#Drawing a sample of n/2 numbers ranging from 1 to n
ind = sample(1:n, size = floor(n/2))
#Dividing OJ into two datasets based on indices
train = xdata[ind,]
test = xdata[-ind,]
head(train)
head(test)
summary(train)
summary(train$Purchase)
#Plotting Purchase-variable
plot(train$Purchase)
plot(train$Purchase~train$WeekofPurchase)
plot(train$Purchase~train$StoreID)
plot(train$Purchase~train$PriceCH)
plot(train$Purchase~train$PriceMM)
plot(train$Purchase~train$DiscCH)
plot(train$Purchase~train$DiscMM)
plot(train$Purchase~train$SpecialCH)
plot(train$Purchase~train$SpecialMM)
#Building model on training data
log_reg1 = glm(Purchase~., data = train, family = binomial())
summary(log_reg1)
#Building model on training data
log_reg1 = glm(Purchase~StoreID + SpecialCH + SpecialMM + PriceDiff + PctDiscMM + PctDiscCH, data = train, family = binomial())
summary(log_reg1)
pred_logreg = predict(log_reg1, newdata = test, type = "response") > 0.5
#Confusion matrix
conf_matrix_logreg = table(test$Purchase, pred_logreg)
#Printing result
conf_matrix_logreg
?predict()
#In probabilities
conf_matrix_logreg_probs = conf_matrix_logreg/nrow(test)
conf_matrix_logreg_probs
accuracy = sum(diag(conf_matrix_logreg_probs))
accuracy
#Loading tree-library
library(tree)
#Fitting the classification tree on the training data
t = tree(Purchase~., data = train)
summary(t)
#Loading tree-library
library(tree)
#Fitting the classification tree on the training data
t = tree(Purchase~., data = train)
summary(t)
#Plotting the tree
plot(t)
text(t, pretty = 0)
#Making predictions
pred_tree = predict(t, newdata = test, type = "response") > 0.5
#Making predictions
pred_tree = predict(t, newdata = test, type = "class") > 0.5
#Making predictions
pred_tree = predict(t, test, type = "class")
#Confusion matrix
table(test$Purchase, pred_tree)
#Confusion matrix
conf_matrix_tree = table(test$Purchase, pred_tree)
#In probabilities
conf_matrix_tree_probs = conf_matrix_tree/nrow(test)
#Printing results
conf_matrix_tree
conf_matrix_tree_probs
#Making predictions
pred_tree = predict(t, test, type = "class")
#Confusion matrix
conf_matrix_tree = table(test$Purchase, pred_tree)
#In probabilities
conf_matrix_tree_probs = conf_matrix_tree/nrow(test)
#Printing results
conf_matrix_tree
conf_matrix_tree_probs
accuracy_tree = sum(diag(conf_matrix_tree_probs))
accuracy_tree
?cv.tree()
pred5 = predict(t, newdata = test, type = "class")
?tree()
new_train = OJ[ind,]
new_test = OJ[-ind,]
ydata = OJ
ydata = OJ
#Repeating changes except keeping LoyalCH
#Store ID could be recoded to a categorical variable
xdata$StoreID = as.factor(xdata$StoreID)
#Special variables are binary
xdata$SpecialCH = as.factor(xdata$SpecialCH)
xdata$SpecialMM = as.factor(xdata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
xdata = xdata[,-17]
#Removing Store7 (column 13) for the same reason
xdata = xdata[,-13]
ydata = OJ
#Repeating changes except keeping LoyalCH
#Store ID could be recoded to a categorical variable
ydata$StoreID = as.factor(ydata$StoreID)
#Special variables are binary
ydata$SpecialCH = as.factor(ydata$SpecialCH)
ydata$SpecialMM = as.factor(ydata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
ydata = ydata[,-17]
#Removing Store7 (column 13) for the same reason
ydata = ydata[,-13]
str(ydata)
ydata = OJ
#Repeating changes except keeping LoyalCH
#Store ID could be recoded to a categorical variable
ydata$StoreID = as.factor(ydata$StoreID)
#Special variables are binary
ydata$SpecialCH = as.factor(ydata$SpecialCH)
ydata$SpecialMM = as.factor(ydata$SpecialMM)
#I will also remove store-variable (column 17), as we already have the store ID-variable
ydata = ydata[,-17]
#Removing Store7 (column 13) for the same reason
ydata = ydata[,-13]
str(ydata)
str(OJ)
str(OJ)
str(ydata)
ydata = OJ
str(ydata)
ydata = OJ
str(ydata)
#Repeating changes except keeping LoyalCH
#Store ID could be recoded to a categorical variable
ydata$StoreID = as.factor(ydata$StoreID)
#Special variables are binary
ydata$SpecialCH = as.factor(ydata$SpecialCH)
ydata$SpecialMM = as.factor(ydata$SpecialMM)
#I will also remove store-variable (column 18 now that we keep LoyalCH), as we already have the store ID-variable
ydata = ydata[,-18]
#Removing Store7 (column 14) for the same reason
ydata = ydata[,-14]
str(ydata)
new_train = ydata[ind,]
new_test = ydata[-ind,]
#Building model on training data
log_reg2 = glm(Purchase~StoreID + SpecialCH + SpecialMM + PriceDiff + PctDiscMM + PctDiscCH + LoyalCH, data = new_train, family = binomial())
summary(log_reg1)
#Building model on training data
log_reg2 = glm(Purchase~StoreID + SpecialCH + SpecialMM + PriceDiff + PctDiscMM + PctDiscCH + LoyalCH, data = new_train, family = binomial())
summary(log_reg2)
#Making predictions
pred_logreg2 = predict(log_reg2, newdata = new_test, type = "response") > 0.5
#Confusion matrix
conf_matrix_logreg2 = table(new_test$Purchase, pred_logreg2)
#In probabilities
conf_matrix_logreg_probs2 = conf_matrix_logreg2/nrow(new_test)
#Printing result
conf_matrix_logreg2
conf_matrix_logreg_probs2
accuracy2 = sum(diag(conf_matrix_logreg_probs2))
accuracy
accuracy2 = sum(diag(conf_matrix_logreg_probs2))
accuracy2
#Fitting the classification tree on the training data
t2 = tree(Purchase~., data = new_train)
summary(t)
#Plotting the tree
plot(t2)
text(t2, pretty = 0)
importance(t2)
#Making predictions
pred_tree2 = predict(t2, new_test, type = "class")
#Confusion matrix
conf_matrix_tree2 = table(new_test$Purchase, pred_tree2)
#In probabilities
conf_matrix_tree_probs2 = conf_matrix_tree2/nrow(new_test)
#Printing results
conf_matrix_tree2
conf_matrix_tree_probs2
accuracy_tree2 = sum(diag(conf_matrix_tree_probs2))
accuracy_tree2
help(OJ)
threshold_matrix = matrix(ncol = 2, nrow = 99, 0)
View(train)
View(test)
View(threshold_matrix)
colnames(threshold_matrix) = c("accuracy", "threshold")
threshold_matrix = matrix(ncol = 2, nrow = 99, 0)
colnames(threshold_matrix) = c("accuracy", "threshold")
for(i in 1:99) {
threshold = i/100
#Making predictions with current threshold
pred = predict(log_reg1, newdata = test, type = "response") > threshold
#Confusion matrix
conf_matrix_probs = table(test$Purchase, pred)/nrow(test)
#Computing accuracy
accuracy = sum(diag(conf_matrix_probs))
#Storing accuracy and threshold in matrix
threshold_matrix[i, 1] = accuracy
threshold_matrix[i, 2] = threshold
}
plot(threshold_matrix)
plot(threshold_matrix$accuracy~threshold_matrix$threshold)
plot(threshold_matrix$accuracy~threshold_matrix$threshold)
accuracy = threshold_matrix$accuracy)
accuracy = threshold_matrix$accuracy
accuracy = threshold_matrix[,1]
accuracy_vec = threshold_matrix[,1]
threshold_vec = threshold_matrix[,2]
plot(accuracy_vec~threshold_vec)
max_accuracy = max(accuracy_vec)
match(max_accuracy, accuracy_vec)
ind_max = match(max_accuracy, accuracy_vec)
max_accuracy = max(accuracy_vec)
max_accuracy
ind_max = match(max_accuracy, accuracy_vec)
best_threshold = threshold_vec[ind_max]
best_threshold
char.data <- data.frame(
person = 1:6,
hair = c("brown", "blonde", "black", "blonde", "blonde", "black"),
height = c(180, 174, 185, 160, 165, 192),
grade = c(1, 2, 3, 2, 3, 1)
)
char.data
#Extract some elements
char.data$hair
char.data[,"hair"]
char.data[,2]
char.data[3,2]
char.data[char.data$hair == "blonde",]
#Replacing a variable
char.data$grade[char.data$grade <= 2] <- 2
char.data$grade
#Declaring a new variable
char.data$new.var <- 6:1
char.data[["new.var"]] <- 6:1
char.data
for(i in 1:5) {
print(i^2)
}
x <- rnorm(100)
hist(x)
?rnorm()
x <- rnorm(100)
hist(x)
rnorm(100) %>%
hist()
library(dplyr)
rnorm(100) %>%
hist()
# - Let's start with a tibble
our.data <- tibble(x = rnorm(100))
our.data %>%
mutate(y = x^2) -> our.data
source("~/NHH/H22/BAN432 Textual Data Analysis/Lecture1 Intro to R.R", echo=TRUE)
our.data
# - select() picks variables based on their names
select(our.data, y)
our.data %>%
select(y)
# - filter() picks cases based on their values
filter(our.data, y > 0)
our.data %>%
filter(y>1)
setwd("~/NHH/H22/BAN400")
